# lmapp Roadmap

## Current Status

**v0.1.0** âœ… Released on PyPI  
- 128/128 tests passing
- 100% test coverage
- MIT-licensed, production-ready

---

## Phase 2: Backend Integration (v0.2.0)

### Ollama Support
- [ ] Detect Ollama installation
- [ ] Execute Ollama service
- [ ] List available models
- [ ] Stream LLM responses in chat

### llamafile Support
- [ ] Auto-download llamafile
- [ ] Execute and manage lifecycle
- [ ] Model selection and switching

### Quality Gates
- [ ] All tests passing
- [ ] Documented error scenarios
- [ ] Health checks and recovery

**Expected Duration:** 2-4 weeks

---

## Phase 3: Chat & UX Improvements

- [ ] Multi-turn conversation history
- [ ] Model management CLI (`lmapp models list`, `lmapp models switch`)
- [ ] Chat export (Markdown, JSON)
- [ ] Configuration UI improvements

**Expected Duration:** 1-2 weeks

---

## Phase 4: Platform Integration (v0.3.0+)

- [ ] Shell customization (bash, zsh)
- [ ] Plugin system
- [ ] Custom prompts and personas
- [ ] API for external tools

**Expected Duration:** 4+ weeks

---

## Community & Feedback Loop

- Gather user feedback from v0.1.0 launch
- Iterate based on real-world usage patterns
- Prioritize most-requested features

---

## How to Contribute

See [CONTRIBUTING.md](./CONTRIBUTING.md) for development guidelines.
